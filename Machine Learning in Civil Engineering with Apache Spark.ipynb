{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Machine Learning in Civil Engineering with Apache Spark</font>\n",
    "\n",
    "#### <font>Development of the AutoML system itself, without the use of specific frameworks applying Machine Learning with Spark MLlib in PySpark.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark and initialize\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Spark Context\n",
    "sc = SparkContext(appName = \"Civil-Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Civil-Engineering</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a06038b400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os data\n",
    "data = spark.read.csv('data/dataset.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of records\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "|cement| slag|flyash|water|superplasticizer|coarseaggregate|fineaggregate|age|csMPa|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1040.0|        676.0| 28|79.99|\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1055.0|        676.0| 28|61.89|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|270|40.27|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|365|41.05|\n",
      "| 198.6|132.4|   0.0|192.0|             0.0|          978.4|        825.5|360| 44.3|\n",
      "| 266.0|114.0|   0.0|228.0|             0.0|          932.0|        670.0| 90|47.03|\n",
      "| 380.0| 95.0|   0.0|228.0|             0.0|          932.0|        594.0|365| 43.7|\n",
      "| 380.0| 95.0|   0.0|228.0|             0.0|          932.0|        594.0| 28|36.45|\n",
      "| 266.0|114.0|   0.0|228.0|             0.0|          932.0|        670.0| 28|45.85|\n",
      "| 475.0|  0.0|   0.0|228.0|             0.0|          932.0|        594.0| 28|39.29|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize the data in the Spark DataFrame pattern\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>90</td>\n",
       "      <td>47.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>380.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>43.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>380.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>28</td>\n",
       "      <td>36.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>28</td>\n",
       "      <td>45.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>475.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>28</td>\n",
       "      <td>39.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0   540.0    0.0     0.0  162.0               2.5           1040.0   \n",
       "1   540.0    0.0     0.0  162.0               2.5           1055.0   \n",
       "2   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "3   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "4   198.6  132.4     0.0  192.0               0.0            978.4   \n",
       "5   266.0  114.0     0.0  228.0               0.0            932.0   \n",
       "6   380.0   95.0     0.0  228.0               0.0            932.0   \n",
       "7   380.0   95.0     0.0  228.0               0.0            932.0   \n",
       "8   266.0  114.0     0.0  228.0               0.0            932.0   \n",
       "9   475.0    0.0     0.0  228.0               0.0            932.0   \n",
       "\n",
       "   fineaggregate  age  csMPa  \n",
       "0          676.0   28  79.99  \n",
       "1          676.0   28  61.89  \n",
       "2          594.0  270  40.27  \n",
       "3          594.0  365  41.05  \n",
       "4          825.5  360  44.30  \n",
       "5          670.0   90  47.03  \n",
       "6          594.0  365  43.70  \n",
       "7          594.0   28  36.45  \n",
       "8          670.0   28  45.85  \n",
       "9          594.0   28  39.29  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data in Pandas format\n",
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cement: double (nullable = true)\n",
      " |-- slag: double (nullable = true)\n",
      " |-- flyash: double (nullable = true)\n",
      " |-- water: double (nullable = true)\n",
      " |-- superplasticizer: double (nullable = true)\n",
      " |-- coarseaggregate: double (nullable = true)\n",
      " |-- fineaggregate: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- csMPa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Automation Module\n",
    "\n",
    "MLlib requires all dataframe input columns to be vectorized. Let's create a Python function that will automate our data preparation work, including vectorization and all the necessary tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's list and remove missing values (if any). We will focus this project on Machine Learning, but always remember to check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing missing values: 1030\n",
      "Number of rows after removing missing values: 1030\n"
     ]
    }
   ],
   "source": [
    "# Separate the missing data (if they exist) and remove them (if they exist)\n",
    "data_with_lines_removed = data.na.drop()\n",
    "print('Number of rows before removing missing values:', data.count())\n",
    "print('Number of rows after removing missing values:', data_with_lines_removed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de preparação dos data\n",
    "def func_module_data_prep(df,\n",
    "                           input_variables,\n",
    "                           output_variable,\n",
    "                           treat_outliers = True,\n",
    "                           standardize_data = True):\n",
    "\n",
    "    # Let's generate a new dataframe, renaming the argument that represents the output variable.\n",
    "    new_df = df.withColumnRenamed(output_variable, 'label')\n",
    "    \n",
    "    # We convert the target variable to numeric type as float (encoding)\n",
    "    if str(new_df.schema['label'].dataType) != 'IntegerType':\n",
    "        new_df = new_df.withColumn(\"label\", new_df[\"label\"].cast(FloatType()))\n",
    "    \n",
    "    # Checklists for variables\n",
    "    numeric_variables = []\n",
    "    categorical_variables = []\n",
    "    \n",
    "    # If you have input variables of type string, convert to numeric type\n",
    "    for columns in input_variables:\n",
    "        \n",
    "        # Check if the variable is of type string\n",
    "        if str(new_df.schema[columns].dataType) == 'StringType':\n",
    "            \n",
    "            # We define the variable with a suffix\n",
    "            new_name_columns = columns + \"_num\"\n",
    "            \n",
    "            # Adicionamos à lista de variáveis categóricas\n",
    "            categorical_variables.append(new_name_columns)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # If it is not a variable of type string, then it is numeric and we add it to the corresponding list\n",
    "            numeric_variables.append(columns)\n",
    "            \n",
    "            # We put the data in the dataframe of indexed variables\n",
    "            df_indexed = new_df\n",
    "            \n",
    "    # If the dataframe has data of type string, we apply indexing\n",
    "    # Check that the list of categorical variables is not empty\n",
    "    if len(categorical_variables) != 0: \n",
    "        \n",
    "        # loop through columns\n",
    "        for columns in new_df:\n",
    "            \n",
    "            # If the variable is of type string, we create, train and apply the indexer\n",
    "            if str(new_df.schema[columns].dataType) == 'StringType':\n",
    "                \n",
    "                # Create the indexer\n",
    "                indexer = StringIndexer(inputCol = columns, outputCol = columns + \"_num\") \n",
    "                \n",
    "                # Train and apply the indexer\n",
    "                df_indexed = indexer.fit(new_df).transform(new_df)\n",
    "    else:\n",
    "        \n",
    "        # If we don't have categorical variables anymore, then we put the data in the indexed variables dataframe\n",
    "        df_indexed = new_df\n",
    "        \n",
    "    # If it is necessary to handle outliers, we will do it now\n",
    "    if treat_outliers == True:\n",
    "        print(\"\\nApplying outlier treatment...\")\n",
    "        \n",
    "        # dictionary\n",
    "        d = {}\n",
    "        \n",
    "        # Quartile dictionary of indexed dataframe variables (numeric variables only)\n",
    "        for col in numeric_variables: \n",
    "            d[col] = df_indexed.approxQuantile(col,[0.01, 0.99], 0.25) \n",
    "        \n",
    "        # Now we apply transformation depending on the distribution of each variable\n",
    "        for col in numeric_variables:\n",
    "            \n",
    "            # We extract asymmetry from the data and use it to handle outliers\n",
    "            skew = df_indexed.agg(skewness(df_indexed[col])).collect() \n",
    "            skew = skew[0][0]\n",
    "            \n",
    "            # We check for asymmetry and then apply:\n",
    "            \n",
    "            # Log transform + 1 if skewness is positive\n",
    "            if skew > 1:\n",
    "                indexed = df_indexed.withColumn(col, log(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] ) + 1).alias(col))\n",
    "                print(\"\\nThe variable \" + col + \" was treated for positive asymmetry (right) with skew =\", skew)\n",
    "                \n",
    "            # Exponential transformation if the skewness is negative\n",
    "            elif skew < -1:\n",
    "                indexed = df_indexed.withColumn(col, \\\n",
    "                exp(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] )).alias(col))\n",
    "                print(\"\\nThe variable \" + col + \" was treated for negative skewness (left) with skew =\", skew)\n",
    "                \n",
    "            # Asymmetry between -1 and 1 we do not need to apply transformation to the data\n",
    "\n",
    "    # Vectorization\n",
    "    \n",
    "    # Final list of attributes\n",
    "    list_attributes = numeric_variables + categorical_variables\n",
    "    \n",
    "    # Create vectorizer for attributes\n",
    "    vectorizer = VectorAssembler(inputCols = list_attributes, outputCol = 'features')\n",
    "    \n",
    "    # Apply the vectorizer to the data set\n",
    "    data_vectorized = vectorizer.transform(df_indexed).select('features', 'label')\n",
    "    \n",
    "    # If the standardize data flag is set to True, then we standardize the data by placing them on the same scale\n",
    "    if standardize_data == True:\n",
    "        print(\"\\nDefaulting the date set to the range 0 to 1...\")\n",
    "        \n",
    "        # Create the scaler\n",
    "        scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"scaledFeatures\")\n",
    "\n",
    "        # Compute the statistics summary and generate the standardizer\n",
    "        global scalerModel\n",
    "        scalerModel = scaler.fit(data_vectorized)\n",
    "\n",
    "        # Defaults variables to range [min, max]\n",
    "        data_standardized = scalerModel.transform(data_vectorized)\n",
    "        \n",
    "        # Generate end date\n",
    "        final_data = data_standardized.select('label', 'scaledFeatures')\n",
    "        \n",
    "        # Rename the columns (required by Spark)\n",
    "        final_data = final_data.withColumnRenamed('scaledFeatures', 'features')\n",
    "        \n",
    "        print(\"\\nProcess concluded!\")\n",
    "\n",
    "    # If the flag is set to False, then we don't standardize the data\n",
    "    else:\n",
    "        print(\"\\nThe data will not be standardized because the flag standardize_data has the value False.\")\n",
    "        final_data = data_vectorized\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we apply the data preparation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of input variables (all but the last one)\n",
    "input_variables = data.columns[:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable\n",
    "output_variable = data.columns[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying outlier treatment...\n",
      "\n",
      "The variable age was treated for positive asymmetry (right) with skew = 3.2644145354168086\n",
      "\n",
      "Defaulting the date set to the range 0 to 1...\n",
      "\n",
      "Process concluded!\n"
     ]
    }
   ],
   "source": [
    "# Apply the function\n",
    "final_data = func_module_data_prep(data, input_variables, output_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                      |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|79.99|[1.0,0.0,0.0,0.3210862619808307,0.07763975155279502,0.6947674418604651,0.20572002007024587,0.07417582417582418]               |\n",
      "|61.89|[1.0,0.0,0.0,0.3210862619808307,0.07763975155279502,0.7383720930232558,0.20572002007024587,0.07417582417582418]               |\n",
      "|40.27|[0.526255707762557,0.3964941569282137,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.739010989010989]                    |\n",
      "|41.05|[0.526255707762557,0.3964941569282137,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,1.0]                                  |\n",
      "|44.3 |[0.22054794520547943,0.3683917640511965,0.0,0.560702875399361,0.0,0.5156976744186046,0.58078273958856,0.9862637362637363]     |\n",
      "|47.03|[0.3744292237442922,0.31719532554257096,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.24450549450549453]|\n",
      "|43.7 |[0.634703196347032,0.2643294379521425,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,1.0]                                  |\n",
      "|36.45|[0.634703196347032,0.2643294379521425,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.07417582417582418]                  |\n",
      "|45.85|[0.3744292237442922,0.31719532554257096,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.07417582417582418]|\n",
      "|39.29|(8,[0,3,5,7],[0.8515981735159817,0.8482428115015974,0.3808139534883721,0.07417582417582418])                                  |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View\n",
    "final_data.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Correlation\n",
    "\n",
    "Let's make sure we don't have multicollinearity before moving on. Remember the following guidelines for the Pearson Correlation Coefficient:\n",
    "\n",
    "- .00-.19 (very weak correlation)\n",
    "- .20-.39 (weak correlation)\n",
    "- .40-.59 (moderate correlation)\n",
    "- .60-.79 (strong correlation)\n",
    "- .80-1.0 (very strong correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the correlation\n",
    "corr_coefficients = Correlation.corr(final_data, 'features', 'pearson').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the result to an array\n",
    "array_corr = corr_coefficients.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.27521591, -0.39746734, -0.08158675,  0.09238617,\n",
       "        -0.10934899, -0.22271785,  0.08194602],\n",
       "       [-0.27521591,  1.        , -0.3235799 ,  0.10725203,  0.04327042,\n",
       "        -0.28399861, -0.28160267, -0.04424602],\n",
       "       [-0.39746734, -0.3235799 ,  1.        , -0.25698402,  0.37750315,\n",
       "        -0.00996083,  0.07910849, -0.15437052],\n",
       "       [-0.08158675,  0.10725203, -0.25698402,  1.        , -0.65753291,\n",
       "        -0.1822936 , -0.45066117,  0.27761822],\n",
       "       [ 0.09238617,  0.04327042,  0.37750315, -0.65753291,  1.        ,\n",
       "        -0.26599915,  0.22269123, -0.19270003],\n",
       "       [-0.10934899, -0.28399861, -0.00996083, -0.1822936 , -0.26599915,\n",
       "         1.        , -0.17848096, -0.00301588],\n",
       "       [-0.22271785, -0.28160267,  0.07910849, -0.45066117,  0.22269123,\n",
       "        -0.17848096,  1.        , -0.1560947 ],\n",
       "       [ 0.08194602, -0.04424602, -0.15437052,  0.27761822, -0.19270003,\n",
       "        -0.00301588, -0.1560947 ,  1.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08194602387182176\n",
      "-0.044246019304454175\n",
      "-0.15437051606792915\n",
      "0.27761822152100296\n",
      "-0.19270002804347258\n",
      "-0.0030158803467436645\n",
      "-0.15609470264758615\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# List the correlation between the attributes and the target variable\n",
    "for iten in array_corr:\n",
    "    print(iten[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division with 70/30 ratio\n",
    "training_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML (Automated Machine Learning) module\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#regression\n",
    "\n",
    "Let's create a function to automate the use of several algorithms. Our function will create, train and evaluate each of them with different combinations of hyperparameters. And then we will choose the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning module\n",
    "def func_ml_module(regression_algorithm):\n",
    "\n",
    "    # Function to get the type of the regression algorithm and create the object instance\n",
    "    # We will use this to automate our process\n",
    "    def func_type_alg(regression_alg):\n",
    "        algorithm = regression_alg\n",
    "        alg_type = type(algorithm).__name__\n",
    "        return alg_type\n",
    "    \n",
    "    # Apply the previous function\n",
    "    alg_type = func_type_alg(regression_algorithm)\n",
    "\n",
    "    # If the algorithm is Linear Regression, enter this block if\n",
    "    if alg_type == \"LinearRegression\":\n",
    "        \n",
    "        # We train the first version of the model without cross-validation\n",
    "        model = regressor.fit(training_data)\n",
    "        \n",
    "        # Model metrics\n",
    "        print('\\033[1m' + \"Linear Regression model Without Cross Validation:\" + '\\033[0m')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Evaluate the model with test date\n",
    "        test_result = model.evaluate(test_data)\n",
    "\n",
    "        # Print model error metrics with test date\n",
    "        print(\"RMSE in Test: {}\".format(test_result.rootMeanSquaredError))\n",
    "        print(\"R2 Coefficient in Test: {}\".format(test_result.r2))\n",
    "        print(\"\")\n",
    "        \n",
    "        # Now let's create the second version of the model with the same algorithm, but using cross validation\n",
    "        \n",
    "         # Prepare the hyperparameter grid\n",
    "        paramGrid = (ParamGridBuilder().addGrid(regressor.regParam, [0.1, 0.01]).build())\n",
    "        \n",
    "        # Create the raters\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        \n",
    "        # Create the Cross Validator\n",
    "        crossval = CrossValidator(estimator = regressor,\n",
    "                                  estimatorParamMaps = paramGrid,\n",
    "                                  evaluator = eval_rmse,\n",
    "                                  numFolds = 3) \n",
    "        \n",
    "        print('\\033[1m' + \"Linear Regression model With Cross Validation:\" + '\\033[0m')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Train the model with cross validation\n",
    "        model = crossval.fit(training_data)\n",
    "        \n",
    "        # Save the best model from version 2\n",
    "        global LR_BestModel \n",
    "        LR_BestModel = model.bestModel\n",
    "                \n",
    "        # Forecasts with test date\n",
    "        forecasts = LR_BestModel.transform(test_data)\n",
    "        \n",
    "        # Rating of the best model\n",
    "        test_result_rmse = eval_rmse.evaluate(forecasts)\n",
    "        print('RMSE em Teste:', test_result_rmse)\n",
    "        \n",
    "        test_result_r2 = eval_r2.evaluate(forecasts)\n",
    "        print('R2 Coefficient in Test:', test_result_r2)\n",
    "        print(\"\")\n",
    "    \n",
    "        # List of columnss to put in summary dataframe\n",
    "        columns = ['Regressor', 'result_RMSE', 'result_R2']\n",
    "        \n",
    "        # Format the results and create the dataframe\n",
    "        \n",
    "        # Format metrics and algorithm name\n",
    "        rmse_str = [str(test_result_rmse)] \n",
    "        r2_str = [str(test_result_r2)] \n",
    "        alg_type = [alg_type] \n",
    "        \n",
    "        # create dataframe\n",
    "        df_result = spark.createDataFrame(zip(alg_type, rmse_str, r2_str), schema = columns)\n",
    "        \n",
    "        # Write the results to the dataframe\n",
    "        df_result = df_result.withColumn('result_RMSE', df_result.result_RMSE.substr(0, 5))\n",
    "        df_result = df_result.withColumn('result_R2', df_result.result_R2.substr(0, 5))\n",
    "        \n",
    "        return df_result\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # We check if the algorithm is the Decision Tree and create the hyperparameter grid\n",
    "        if alg_type in(\"DecisionTreeRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.maxBins, [10, 20, 40]).build())\n",
    "\n",
    "        # We check if the algorithm is Random Forest and create the hyperparameter grid\n",
    "        if alg_type in(\"RandomForestRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.numTrees, [5, 20]).build())\n",
    "\n",
    "        # We check if the algorithm is GBT and create the hyperparameter grid\n",
    "        if alg_type in(\"GBTRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder() \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20]) \\\n",
    "                         .addGrid(regressor.maxIter, [10, 15])\n",
    "                         .build())\n",
    "            \n",
    "        # We check if the algorithm is Isotonic \n",
    "        if alg_type in(\"IsotonicRegression\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.isotonic, [True, False]).build())\n",
    "\n",
    "        # Create the raters\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        \n",
    "        # Prepare the Cross Validator\n",
    "        crossval = CrossValidator(estimator = regressor,\n",
    "                                  estimatorParamMaps = paramGrid,\n",
    "                                  evaluator = eval_rmse,\n",
    "                                  numFolds = 3) \n",
    "        \n",
    "        # Train the model using cross validation\n",
    "        model = crossval.fit(training_data)\n",
    "        \n",
    "        # Extract the best model\n",
    "        BestModel = model.bestModel\n",
    "\n",
    "        # Summary of each model\n",
    "        \n",
    "        # Model metrics\n",
    "        if alg_type in(\"DecisionTreeRegressor\"):\n",
    "            \n",
    "            # global variable\n",
    "            global DT_BestModel \n",
    "            DT_BestModel = model.bestModel\n",
    "            \n",
    "            # Forecasts with test date\n",
    "            forecasts_DT = DT_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"Decision Tree model With Cross Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(forecasts_DT)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(forecasts_DT)\n",
    "            print('R2 Coefficient in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Model metrics\n",
    "        if alg_type in(\"RandomForestRegressor\"):\n",
    "            \n",
    "            # global variable\n",
    "            global RF_BestModel \n",
    "            RF_BestModel = model.bestModel\n",
    "            \n",
    "            # Predictions with test date\n",
    "            forecasts_RF = RF_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"model RandomForest With Cross Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(forecasts_RF)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(forecasts_RF)\n",
    "            print('R2 Coefficient in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Model metrics\n",
    "        if alg_type in(\"GBTRegressor\"):\n",
    "\n",
    "            # global variable\n",
    "            global GBT_BestModel \n",
    "            GBT_BestModel = model.bestModel\n",
    "            \n",
    "            # Forecasts with test date\n",
    "            forecasts_GBT = GBT_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"Gradient-Boosted Tree (GBT) model With Cross Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(forecasts_GBT)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(forecasts_GBT)\n",
    "            print('Coefficient R2 in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Model metrics\n",
    "        if alg_type in(\"IsotonicRegression\"):\n",
    "\n",
    "            # global variable\n",
    "            global ISO_BestModel \n",
    "            ISO_BestModel = model.bestModel\n",
    "            \n",
    "            # Predictions with test date\n",
    "            forecasts_ISO = ISO_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"model Isotonic With Cross Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(forecasts_ISO)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(forecasts_ISO)\n",
    "            print('Coefficient R2  in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "                    \n",
    "        # List of columnss to put in summary dataframe\n",
    "        columns = ['Regressor', 'result_RMSE', 'result_R2']\n",
    "        \n",
    "        # Make predictions with test date\n",
    "        forecasts = model.transform(test_data)\n",
    "        \n",
    "        # Evaluates the model to save the result\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        rmse = eval_rmse.evaluate(forecasts)\n",
    "        rmse_str = [str(rmse)]\n",
    "        \n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        r2 = eval_r2.evaluate(forecasts)\n",
    "        r2_str = [str(r2)]\n",
    "         \n",
    "        alg_type = [alg_type] \n",
    "        \n",
    "        # Create the dataframe\n",
    "        df_result = spark.createDataFrame(zip(alg_type, rmse_str, r2_str), schema = columns)\n",
    "        \n",
    "        # Write the result to the dataframe\n",
    "        df_result = df_result.withColumn('result_RMSE', df_result.result_RMSE.substr(0, 5))\n",
    "        df_result = df_result.withColumn('result_R2', df_result.result_R2.substr(0, 5))\n",
    "        \n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we run the Machine Learning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of algorithms\n",
    "regressors = [LinearRegression(),\n",
    "               DecisionTreeRegressor(),\n",
    "               RandomForestRegressor(),\n",
    "               GBTRegressor(),\n",
    "               IsotonicRegression()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columnss and values\n",
    "columns = ['Regressor', 'result_RMSE', 'result_R2']\n",
    "values = [(\"N/A\", \"N/A\", \"N/A\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the summary table\n",
    "df_training_results = spark.createDataFrame(values, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression model Without Cross Validation:\u001b[0m\n",
      "\n",
      "RMSE in Test: 10.92429904027715\n",
      "R2 Coefficient in Test: 0.5202863509688556\n",
      "\n",
      "\u001b[1mLinear Regression model With Cross Validation:\u001b[0m\n",
      "\n",
      "RMSE em Teste: 10.919263403124427\n",
      "R2 Coefficient in Test: 0.5207285042101175\n",
      "\n",
      "\u001b[1mDecision Tree model With Cross Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 8.057115537446128\n",
      "R2 Coefficient in Test: 0.7390519245046017\n",
      "\n",
      "\u001b[1mmodel RandomForest With Cross Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 7.063163259381452\n",
      "R2 Coefficient in Test: 0.79946351195338\n",
      "\n",
      "\u001b[1mGradient-Boosted Tree (GBT) model With Cross Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 6.928840122411529\n",
      "Coefficient R2 in Test: 0.8070183584170918\n",
      "\n",
      "\u001b[1mmodel Isotonic With Cross Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 13.647380006581942\n",
      "Coefficient R2  in Test: 0.25132473587014537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for regressor in regressors:\n",
    "    \n",
    "    # For each regressor get the result\n",
    "    result_model = func_ml_module(regressor)\n",
    "    \n",
    "    # Save the results\n",
    "    df_training_results = df_training_results.union(result_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return lines other than N/A\n",
    "df_training_results = df_training_results.where(\"Regressor!='N/A'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------+---------+\n",
      "|Regressor            |result_RMSE|result_R2|\n",
      "+---------------------+-----------+---------+\n",
      "|LinearRegression     |10.91      |0.520    |\n",
      "|DecisionTreeRegressor|8.057      |0.739    |\n",
      "|RandomForestRegressor|7.063      |0.799    |\n",
      "|GBTRegressor         |6.928      |0.807    |\n",
      "|IsotonicRegression   |13.64      |0.251    |\n",
      "+---------------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprime\n",
    "df_training_results.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The GBT model showed the best overall performance and will be used in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions with the Trained model\n",
    "\n",
    "To make predictions with the trained model, let's prepare a record with new data.\n",
    "\n",
    "- Cement: 540\n",
    "- Blast Furnace Slag: 0\n",
    "- Fly Ash: 0\n",
    "- Water: 162\n",
    "- Superplasticizer: 2.5\n",
    "- Coarse Aggregate: 1040\n",
    "- Fine Aggregate: 676\n",
    "- Age: 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of input values\n",
    "values = [(540,0.0,0.0,162,2.5,1040,676,28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "column_names = data.columns\n",
    "column_names = column_names[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind values to column names\n",
    "new_datas = spark.createDataFrame(values, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the same transformation applied in the columns age as in the data preparation.\n",
    "new_datas = new_datas.withColumn(\"age\", log(\"age\") +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de atributos\n",
    "list_attributes = [\"cement\",\n",
    "                   \"slag\",\n",
    "                   \"flyash\",\n",
    "                   \"water\",\n",
    "                   \"superplasticizer\",\n",
    "                   \"coarseaggregate\",\n",
    "                   \"fineaggregate\",\n",
    "                   \"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "assembler = VectorAssembler(inputCols = list_attributes, outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to vector\n",
    "new_datas = assembler.transform(new_datas).select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data (same transformation applied to training data)\n",
    "new_datas_scaled = scalerModel.transform(new_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the resulting columns\n",
    "new_datas_final = new_datas_scaled.select('scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns (MLlib requirement)\n",
    "new_datas_final = new_datas_final.withColumnRenamed('scaledFeatures','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasts with new data using the best performing model\n",
    "forecasts_new_datas = GBT_BestModel.transform(new_datas_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            features|       prediction|\n",
      "+--------------------+-----------------+\n",
      "|[1.0,0.0,0.0,0.32...|38.93298775991308|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "forecasts_new_datas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer: \n",
    "A good part of this project was largely done in the Data Science Academy, Big Data Real-Time Analytics with Python and Spark course (part of the Data Scientist training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
